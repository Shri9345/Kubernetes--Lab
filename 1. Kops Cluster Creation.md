## Cluster Setup using Kops

### Task-1:  Launching EC2 instances and Connecting to EC2 Instances using SSH

* Manually Launch a `t2.micro` instance with OS version as `Ubuntu 24.04 LTS` in `North Virginia (us-east-1)` Region.
* Use tag "`Name:Kops-Server`"
* Create a new Keypair with the Name `Kops-Keypair-YourName`
* In security groups, include ports `22 (SSH),` `80 (HTTP),` 443 (HTTPs) and `8080-8090`.
* Configure Storage: 10 GiB
* Launch the Instance.
* Once Launched, Connect to the Instance using `MobaXterm` or `Putty` or `EC2 Instance Connect` with username "`ubuntu`".

### Task 2: Create an IAM role

* Create an IAM role named "kops-admin-role" with the "AdministratorAccess" policy attached.
* Now, associate the IAM role "kops-admin-role" with your EC2 instance named "kops" by following these steps:

  * Navigate to the EC2 console and select the "kops" instance.

  * Click on "Action" > "Security" > "Modify IAM role."

  * Search for the "kops-admin-role" role, select it, and click "Update IAM role."

### Task 3: Setting up a Kubernetes Cluster
Set the hostname to "kops"
```
sudo hostnamectl set-hostname kops
```
Open a new bash shell
```
bash
```
Now, Create a File with name `kops.sh` and Copy paste the below Kops script and then execute.
```
vi kops.sh
```
Copy and save the below script

<details>
  <summary>Click to copy the script</summary>

```
#!/bin/bash

echo "Let's get started with Kubernetes cluster creation using KOPS!"
echo "Enter your name:"
read username
lower_username=$(echo -e "$username" | sed 's/ //g' | tr '[:upper:]' '[:lower:]')
date_now=$(date "+%F-%H-%m")
clname="${lower_username}-${date_now}.k8s.local"
echo "Your Kubernetes cluster name will be $clname"

TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600") 

az=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/placement/availability-zone)
region=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/placement/region)

sudo sed -i "/\$nrconf{restart}/d" /etc/needrestart/needrestart.conf
echo "\$nrconf{restart} = 'a';" | sudo tee -a /etc/needrestart/needrestart.conf
export DEBIAN_FRONTEND=noninteractive
export NEEDRESTART_MODE=a

sudo apt update -y
sudo apt install nano curl python3-pip -y
sudo snap install aws-cli --classic

curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.25.2/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl

curl -LO https://github.com/kubernetes/kops/releases/download/v1.25.2/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops

ssh-keygen -t rsa -N "" -f "$HOME/.ssh/id_rsa" -q <<<y >/dev/null 2>&1

aws s3 mb "s3://$clname" --region "$region"

export KOPS_STATE_STORE="s3://$clname"

kops create cluster --node-count=2 --master-size="t2.medium" --node-size="t2.medium" --master-volume-size=20 --node-volume-size=20 --zones "$az" --name "$clname" --ssh-public-key ~/.ssh/id_rsa.pub --yes

kops update cluster "$clname" --yes
echo "export KOPS_STATE_STORE=s3://$clname" >> /home/ubuntu/.bashrc
source /home/ubuntu/.bashrc

kops export kubecfg --admin

for ((x = 0; x < 30; x++)); do
  echo "Validating Cluster"
  kops validate cluster >status.txt 2>/dev/null
  if grep -q "is ready" status.txt; then
    echo "Your Cluster is now ready!"
    break
  else
    sleep 20
    echo "x: $x"
  fi
done

kops delete cluster --name "$clname" >delete.txt
sg1=$(grep "sg-" delete.txt | awk '{print $3}' | sed -n '1p')
sg2=$(grep "sg-" delete.txt | awk '{print $3}' | sed -n '2p')
sg3=$(grep "sg-" delete.txt | awk '{print $3}' | sed -n '3p')
aws ec2 authorize-security-group-ingress --group-id "$sg1" --protocol tcp --port 30000-32767 --cidr 0.0.0.0/0 --region "$region"
aws ec2 authorize-security-group-ingress --group-id "$sg2" --protocol tcp --port 30000-32767 --cidr 0.0.0.0/0 --region "$region"
aws ec2 authorize-security-group-ingress --group-id "$sg3" --protocol tcp --port 30000-32767 --cidr 0.0.0.0/0 --region "$region"

echo "export KOPS_STATE_STORE=s3://$clname" >delete-kops.sh
echo "kops delete cluster --name $clname --yes" >>delete-kops.sh
chmod +x delete-kops.sh
chown ubuntu:ubuntu delete-kops.sh

echo "Creating Kubernetes Dashboard"

cat >kubernetes-dashboard.yaml <<EOF
<Your Kubernetes Dashboard YAML content here>
EOF

kubectl apply -f kubernetes-dashboard.yaml

echo "Retrieving URL of SVC"
url1="https://$(kubectl get nodes -o wide -n kubernetes-dashboard | awk '{print \$7}' | sed -n '2p'):32000"
url2="https://$(kubectl get nodes -o wide -n kubernetes-dashboard | awk '{print \$7}' | sed -n '3p'):32000"
url3="https://$(kubectl get nodes -o wide -n kubernetes-dashboard | awk '{print \$7}' | sed -n '4p'):32000"

{
  echo "********        HERE ARE THE DETAILS REQUIRED        ********"
  echo "******** You can use any one of the below given URLs ********"
  echo "First URL is: $url1"
  echo "Second URL is: $url2"
  echo "Third URL is: $url3"
} >token.txt

echo "Creating Token"
kubectl -n kube-system describe secret "$(kubectl -n kube-system get secret | grep kops-admin | awk '{print \$1}')" | grep token: | awk '{print \$2}' >>token.txt
echo "******************          END          ******************" >>token.txt

cat token.txt
```

</details>

Exceute the below command to Retrieve information about the existing clusters
```
. kops.sh
```
Get the kops Cluster Information
```
kops get cluster
```
Get information about the Kubernetes nodes in the cluster
```
kubectl get nodes
```
---
### (Optional Step-1)
**To scale-out/scale-in the Auto Scaling Size, execute the below command.** 
1. To Scale (Up/down) the Master Node, execute below command.
```
aws autoscaling update-auto-scaling-group --auto-scaling-group-name master-us-east-1a.masters.kiran-2024-07-15-08-07.k8s.local --min-size 0 --max-size 0 --desired-capacity 0 --region us-east-1
```
2. To Scale (Up/down) the Slave Nodes, execute below command.
```
aws autoscaling update-auto-scaling-group --auto-scaling-group-name nodes-us-east-1a.kiran-2024-07-15-08-07.k8s.local --min-size 3 --max-size 5 --desired-capacity 5 --region us-east-1
```
#### Note:
* Replace the `nodes-us-east-2a.kiran-2024-03-19-16-03.k8s.local` with the `auto-scaling group's `master or worker nodes` and also replace `region` with your region.`
* Make sure the `Max capacity` of the `Auto Scaling Group` is `above or equal` to the `Desired capacity.`
---
### (Optional Step-2)
**Once Scaling (Up/Down) is done and If you are unable to retrieve the data of nodes or the cluster, you can use the following command.** 
```
kops get cluster
```
```
kops export kubeconfig <cluster-name> --admin
```
```
kops validate cluster
```
Get information about the Kubernetes nodes in the cluster
```
kubectl get nodes
```
---
### (Optional Step-3)
**To `Delete the Entire Cluster` execute below command.**
```
kops delete cluster --name <cluster-name> --state s3://<cluster-name> --yes
```
### ============================= END of LAB =============================
